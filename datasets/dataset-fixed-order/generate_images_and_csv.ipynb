{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generates images for processing by the VLM\n",
    "Prereq for running this script: you need the datasets for the respective generators\n",
    "  \n",
    "Download and unzip google's emoji dataset here:\n",
    "* https://github.com/googlefonts/noto-emoji/\n",
    " \n",
    "Download and unzip twitter's emoji dataset here:\n",
    "* https://github.com/jdecked/twemoji/\n",
    "\n",
    "### The folder containing the images need to be at the same level at this script (store under `dataset_following_elco_split`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import os\n",
    "import cv2\n",
    "import grapheme\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_SIZE = 224 # it's a square image\n",
    "MAX_EMOJIS_PER_ROW = 3\n",
    "MAX_EMOJI_SEQUENCE_LENGTH = MAX_EMOJIS_PER_ROW * MAX_EMOJIS_PER_ROW\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emoji_str_from_description(desc_str):\n",
    "    desc_str = desc_str[8:-1] # get rid of \"This is\" at start and \".\" at end \n",
    "    desc_list = desc_str.split(' [EM] ')\n",
    "    return ''.join([emoji.emojize(f\":{desc}:\") for desc in desc_list])\n",
    "\n",
    "def get_png_image_from_local_repo(emoji, image_dir, filename_getter_fn, extension=\"png\"):\n",
    "  filename = filename_getter_fn(emoji, extension)\n",
    "  path = f'{image_dir}/{filename}'\n",
    "  if not os.path.exists(path):\n",
    "    print(f\"Couldn't find image for {emoji} at {path}\")\n",
    "    return None\n",
    "  img = cv2.imread(path, cv2.IMREAD_COLOR)  # Use only RGB channels\n",
    "  img_new_size = OUTPUT_SIZE // MAX_EMOJIS_PER_ROW\n",
    "  img = cv2.resize(img, (img_new_size, img_new_size))\n",
    "  return img\n",
    "\n",
    "def emoji_to_noto_filename(emoji, extension=\"png\"):\n",
    "    # Step 1: Get the Unicode code points of the emoji.\n",
    "    codepoints = [f\"U{ord(char):04X}\" for char in emoji]\n",
    "    # Step 2: Join the code points with underscores for ZWJ (Zero-Width Joiner) support.\n",
    "    # Replace the \"U\" prefix with a lowercase \"u\" and remove the \"+\" symbol.\n",
    "    file_name_parts = [f\"u{codepoints[0][1:].lower()}\"]  # First code point with \"u\"\n",
    "    for codepoint in codepoints[1:]:\n",
    "        str = codepoint[1:].lower()\n",
    "        if str == 'fe0f': # Remove the variation selector\n",
    "          continue\n",
    "        file_name_parts.append(codepoint[1:].lower())  # Following code points without \"u\"\n",
    "    \n",
    "    # Step 3: Construct the filename (e.g., emoji_u1f9cf_200d_2640.png)\n",
    "    file_name = \"_\".join(file_name_parts)\n",
    "    return f\"emoji_{file_name}.{extension}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count the lengths and graph it\n",
    "# elco_df['length'] = elco_df['EM'].apply(lambda x: len(list(grapheme.graphemes(x))))\n",
    "# elco_df['length'].hist(bins=elco_df['length'].max())\n",
    "# elco_df['length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main method to generate image datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipped_indices = {\n",
    "  'train': set(),\n",
    "  'test': set(),\n",
    "  'val': set()\n",
    "}\n",
    "def generate_img_folder(folder_type, csv_file_path, local_image_dir, output_folder, filename_generator_fn, randomise=False):\n",
    "  df = pd.read_csv(csv_file_path)\n",
    "  df['EM'] = df['sent1'].apply(emoji_str_from_description)\n",
    "\n",
    "  if os.path.exists(output_folder):\n",
    "    # delete the directory\n",
    "    shutil.rmtree(output_folder)\n",
    "\n",
    "  os.makedirs(output_folder)\n",
    "\n",
    "  undone = 0\n",
    "\n",
    "  i = -1\n",
    "  for text in df['EM']:\n",
    "    i += 1\n",
    "    units = list(grapheme.graphemes(text))\n",
    "    units = [unit for unit in units if unit != ',' and unit != ' '] # don't ask me why they're there\n",
    "    if len(units) > MAX_EMOJI_SEQUENCE_LENGTH:\n",
    "      print(f\"Skipping {text} because it's too long\")\n",
    "      skipped_indices[folder_type].add(i)\n",
    "      undone += 1\n",
    "      continue\n",
    "    \n",
    "    output_filename = f'{output_folder}/{i}.png'\n",
    "    \n",
    "    x = 0\n",
    "    y = 0\n",
    "    # Make a white image of size OUTPUT_SIZE x OUTPUT_SIZE\n",
    "    canvas = np.zeros((OUTPUT_SIZE, OUTPUT_SIZE, 3), dtype=np.uint8)  # RGB image\n",
    "    \n",
    "    img_new_size = OUTPUT_SIZE // MAX_EMOJIS_PER_ROW\n",
    "\n",
    "    if randomise:\n",
    "      while len(units) < MAX_EMOJI_SEQUENCE_LENGTH:\n",
    "        units.append(' ')\n",
    "      np.random.shuffle(units)\n",
    "\n",
    "    generated = True\n",
    "    # Generate the image\n",
    "    for j in range(len(units)):\n",
    "      unit = units[j]\n",
    "      if unit == ' ':\n",
    "        continue\n",
    "      img = get_png_image_from_local_repo(unit, local_image_dir, filename_generator_fn)\n",
    "      if img is None:\n",
    "        skipped_indices[folder_type].add(i)\n",
    "        undone += 1\n",
    "        generated = False\n",
    "        break\n",
    "\n",
    "      # Ensure img is in RGB before placing it on the canvas\n",
    "      if img.shape[2] == 4:  # If the image is RGBA, convert it to RGB\n",
    "          img = img[..., :3]\n",
    "\n",
    "      # Write the img to canvas starting at x, y\n",
    "      x_pos = j % MAX_EMOJIS_PER_ROW\n",
    "      y_pos = j // MAX_EMOJIS_PER_ROW\n",
    "      x = x_pos * img_new_size\n",
    "      y = y_pos * img_new_size\n",
    "      canvas[y:y+img.shape[0], x:x+img.shape[1]] = img\n",
    "    \n",
    "    if generated:\n",
    "      cv2.imwrite(output_filename, canvas)\n",
    "  \n",
    "  print(f\"Number of rows skipped in {folder_type}: {undone}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling the main method for each split (val, test, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find image for üá¶üáΩ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e6_1f1fd.png\n",
      "Couldn't find image for üá∫üá∏ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fa_1f1f8.png\n",
      "Couldn't find image for üáªüá™ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fb_1f1ea.png\n",
      "Couldn't find image for üá¶üáΩ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e6_1f1fd.png\n",
      "Couldn't find image for üá∫üá∏ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fa_1f1f8.png\n",
      "Couldn't find image for üá´üá∑ at googlefonts-noto-emoji-main-png-512/emoji_u1f1eb_1f1f7.png\n",
      "Couldn't find image for üá∫üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fa_1f1f3.png\n",
      "Couldn't find image for üáªüá™ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fb_1f1ea.png\n",
      "Couldn't find image for üá¶üá´ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e6_1f1eb.png\n",
      "Couldn't find image for üáªüá™ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fb_1f1ea.png\n",
      "Undone: 10\n",
      "Skipping üë¨üë¨üë¨üë¨üë¨üë¨üë¨üë¨üë¨üë¨üë¨ because it's too long\n",
      "Couldn't find image for üá∑üá∫ at googlefonts-noto-emoji-main-png-512/emoji_u1f1f7_1f1fa.png\n",
      "Skipping üî•üòê:pouting_face: because it's too long\n",
      "Skipping :pouting_face::pouting_face: because it's too long\n",
      "Skipping :pouting_face:üò§ü§¨ because it's too long\n",
      "Skipping :pouting_face:üò§ because it's too long\n",
      "Skipping üî•:pouting_face: because it's too long\n",
      "Skipping :pouting_face:ü§¨ because it's too long\n",
      "Skipping :pouting_face:ü§¨üó£Ô∏è because it's too long\n",
      "Skipping ü§¨üí¨:pouting_face: because it's too long\n",
      "Skipping üßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÇÔ∏èüßç‚Äç‚ôÇÔ∏èüïç because it's too long\n",
      "Skipping :pouting_face:üëÆ because it's too long\n",
      "Skipping üè∞üëëüëéüëéüëéüë¨üë≠:pouting_face: because it's too long\n",
      "Skipping üëçüò†:pouting_face:üò§ because it's too long\n",
      "Skipping :pouting_face:üëç because it's too long\n",
      "Couldn't find image for üá∑üá∫ at googlefonts-noto-emoji-main-png-512/emoji_u1f1f7_1f1fa.png\n",
      "Skipping :pouting_face:üò§ because it's too long\n",
      "Skipping üî•üòê:pouting_face: because it's too long\n",
      "Skipping üî•üòê:pouting_face: because it's too long\n",
      "Skipping üî•:pouting_face: because it's too long\n",
      "Skipping üî•üòê:pouting_face: because it's too long\n",
      "Skipping üî•üòê:pouting_face: because it's too long\n",
      "Skipping ü§¨üí¨:pouting_face: because it's too long\n",
      "Skipping üè∞üëëüëéüëéüëéüë¨üë≠:pouting_face: because it's too long\n",
      "Skipping üëçüò†:pouting_face:üò§ because it's too long\n",
      "Undone: 25\n",
      "Skipping ü§¨:pouting_face: because it's too long\n",
      "Couldn't find image for üá¶üáΩ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e6_1f1fd.png\n",
      "Skipping üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶üì¶‚úñÔ∏èüíØ because it's too long\n",
      "Skipping üçèüçéüçêüçäü•îüç†ü•êü•Øüçûü´ìüçëü••ü•ù because it's too long\n",
      "Skipping üçèüçåü•êüåÆü•™ü•®üßÄü•öü•ûü•¨üå∂Ô∏èüç¢üßãüå∞üç™üéÇü•Çüç® because it's too long\n",
      "Skipping üí∏üè¶üíµü™ôüí∞üíπüèßüí≤üí≥ü§ë because it's too long\n",
      "Skipping üòë:pinata:üß∏ because it's too long\n",
      "Couldn't find image for : at googlefonts-noto-emoji-main-png-512/emoji_u003a.png\n",
      "Skipping üò¥üõå:zzz::zzz: because it's too long\n",
      "Couldn't find image for : at googlefonts-noto-emoji-main-png-512/emoji_u003a.png\n",
      "Skipping üò¥üò¥:zzz::zzz: because it's too long\n",
      "Couldn't find image for üá∏üá¨ at googlefonts-noto-emoji-main-png-512/emoji_u1f1f8_1f1ec.png\n",
      "Couldn't find image for : at googlefonts-noto-emoji-main-png-512/emoji_u003a.png\n",
      "Couldn't find image for üáªüá™ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fb_1f1ea.png\n",
      "Couldn't find image for üá´üá∑ at googlefonts-noto-emoji-main-png-512/emoji_u1f1eb_1f1f7.png\n",
      "Couldn't find image for üá∫üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fa_1f1f3.png\n",
      "Couldn't find image for üáªüá™ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fb_1f1ea.png\n",
      "Couldn't find image for üáßüá± at googlefonts-noto-emoji-main-png-512/emoji_u1f1e7_1f1f1.png\n",
      "Couldn't find image for üá¶üá´ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e6_1f1eb.png\n",
      "Couldn't find image for üáÆüá¥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1ee_1f1f4.png\n",
      "Couldn't find image for üá¨üáß at googlefonts-noto-emoji-main-png-512/emoji_u1f1ec_1f1e7.png\n",
      "Couldn't find image for üáªüá™ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fb_1f1ea.png\n",
      "Couldn't find image for üá®üáø at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1ff.png\n",
      "Couldn't find image for üáÆüá¥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1ee_1f1f4.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Skipping :pouting_face:üßπüò≠ because it's too long\n",
      "Skipping ü§¨:pouting_face: because it's too long\n",
      "Couldn't find image for üá¶üáΩ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e6_1f1fd.png\n",
      "Skipping üçèüçåü•êüåÆü•™ü•®üßÄü•öü•ûü•¨üå∂Ô∏èüç¢üßãüå∞üç™üéÇü•Çüç® because it's too long\n",
      "Skipping üí∏üè¶üíµü™ôüí∞üíπüèßüí≤üí≥ü§ë because it's too long\n",
      "Skipping üëçüò†:pouting_face:üò§ because it's too long\n",
      "Skipping üëçüò†:pouting_face:üò§ because it's too long\n",
      "Skipping üòë:pinata:üß∏ because it's too long\n",
      "Skipping üòë:pinata:üß∏ because it's too long\n",
      "Skipping üòë:pinata:üß∏ because it's too long\n",
      "Couldn't find image for : at googlefonts-noto-emoji-main-png-512/emoji_u003a.png\n",
      "Skipping üò¥üõå:zzz::zzz: because it's too long\n",
      "Skipping üò¥üò¥:zzz::zzz: because it's too long\n",
      "Couldn't find image for : at googlefonts-noto-emoji-main-png-512/emoji_u003a.png\n",
      "Couldn't find image for : at googlefonts-noto-emoji-main-png-512/emoji_u003a.png\n",
      "Couldn't find image for üáªüá™ at googlefonts-noto-emoji-main-png-512/emoji_u1f1fb_1f1ea.png\n",
      "Couldn't find image for üá®üáø at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1ff.png\n",
      "Couldn't find image for üá®üáø at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1ff.png\n",
      "Couldn't find image for üá®üáø at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1ff.png\n",
      "Couldn't find image for üáÆüá¥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1ee_1f1f4.png\n",
      "Couldn't find image for üá¨üáß at googlefonts-noto-emoji-main-png-512/emoji_u1f1ec_1f1e7.png\n",
      "Couldn't find image for üá´üá∑ at googlefonts-noto-emoji-main-png-512/emoji_u1f1eb_1f1f7.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Couldn't find image for üá®üá≥ at googlefonts-noto-emoji-main-png-512/emoji_u1f1e8_1f1f3.png\n",
      "Undone: 57\n"
     ]
    }
   ],
   "source": [
    "output_folder = os.path.join('generated_img_dataset', 'google_dataset')\n",
    "local_image_dir = os.path.join('googlefonts-noto-emoji-main-png-512')\n",
    "\n",
    "datasets = {\n",
    "  'val': os.path.join('.', 'original_ELCo_dataset', 'val.csv'),\n",
    "  'test': os.path.join('.', 'original_ELCo_dataset', 'test.csv'),\n",
    "  'train': os.path.join('.', 'original_ELCo_dataset', 'train.csv')\n",
    "}\n",
    "\n",
    "for folder_type, csv_file_path in datasets.items():\n",
    "  output_folder = os.path.join('generated_img_dataset', f'{folder_type}_google')\n",
    "  generate_img_folder(folder_type, csv_file_path, local_image_dir, output_folder, emoji_to_noto_filename, randomise=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing utility functions\n",
    "* `list[emoji]` -> `str` \n",
    "* `str` -> `list[emoji]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòãüçû\n",
      "U+1F3B7\n",
      "necktie [EM] chart_increasing.\n",
      "[':necktie:', ':chart_increasing:']\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import ast\n",
    "\n",
    "def emoji_to_unicode(emoji_str):\n",
    "    return ' '.join([f\"U+{ord(char):X}\" for char in emoji_str])\n",
    "\n",
    "def process_emoji_list_to_str(emoji_list):\n",
    "    desc_processed = ' [EM] '.join(desc.strip(':') for desc in emoji_list)\n",
    "    return f\"{desc_processed}.\"\n",
    "\n",
    "def unprocess_emoji_list_from_str(emoji_str):\n",
    "    s = emoji_str[:-1].split(' [EM] ')\n",
    "    return [f\":{desc}:\" for desc in s]\n",
    "\n",
    "def emoji_str_from_description(desc_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a description string containing emoji descriptions into a string of actual emojis.\n",
    "    Args:\n",
    "        desc_str (str): A string containing emoji descriptions separated by ' [EM] '.\n",
    "                        The string is expected to start with \"This is\" and end with a period (\".\").\n",
    "\n",
    "    Returns:\n",
    "        str: A string of emojis corresponding to the descriptions in the input string.\n",
    "\n",
    "    Example:\n",
    "        >>> emoji_str_from_description('This is face_savoring_food [EM] bread.')\n",
    "        'üòãüçû'\n",
    "    \"\"\"\n",
    "    desc_str = desc_str[8:-1]  # Remove \"This is\" at the start and \".\" at the end\n",
    "    desc_list = desc_str.split(' [EM] ')  # Split the string into a list of descriptions\n",
    "    return ''.join([emoji.emojize(f\":{desc}:\") for desc in desc_list])  # Convert descriptions to emojis\n",
    "\n",
    "# write some tests for processing and unprocessingüìà\n",
    "print(emoji_str_from_description('This is face_savoring_food [EM] bread.'))\n",
    "print(emoji_to_unicode('üé∑'))\n",
    "print(process_emoji_list_to_str([':necktie:', ':chart_increasing:']))\n",
    "print(unprocess_emoji_list_from_str(process_emoji_list_to_str([':necktie:', ':chart_increasing:'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import ast\n",
    "import os\n",
    "\n",
    "def generate_csv(folder_type, input_file_path, output_file_path, img_folder):\n",
    "    with open(input_file_path, newline='', encoding='utf-8') as fin, \\\n",
    "        open(output_file_path, 'w', newline='', encoding='utf-8') as fout:\n",
    "\n",
    "        reader = csv.DictReader(fin)\n",
    "        fieldnames = ['EM', 'EN', 'unicode', 'label', 'strategy', 'image'] # needs to have the same fieldnames in the output csv\n",
    "        writer = csv.DictWriter(fout, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        i = 0\n",
    "        for row in reader:\n",
    "            if i in skipped_indices[folder_type]:\n",
    "                i += 1\n",
    "                continue\n",
    "            em = emoji_str_from_description(row['sent1'])\n",
    "            en = ' '.join(row['sent2'].split()[2:])[:-1] \n",
    "            writer.writerow({\n",
    "                'EM': em,\n",
    "                'EN': en,\n",
    "                'unicode': emoji_to_unicode(em),  # Added this to match fieldnames\n",
    "                'label': row['label'],\n",
    "                'strategy': row['strategy'],\n",
    "                'image': os.path.join(img_folder, f\"{i}.png\") \n",
    "            })\n",
    "\n",
    "            i += 1\n",
    "\n",
    "    print(f\"Conversion complete! Output saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! Output saved to ./generated_img_dataset/train.csv\n",
      "Conversion complete! Output saved to ./generated_img_dataset/test.csv\n",
      "Conversion complete! Output saved to ./generated_img_dataset/val.csv\n"
     ]
    }
   ],
   "source": [
    "csvs_to_generate = [\n",
    "    (\n",
    "     'train',\n",
    "     os.path.join('.', 'original_ELCo_dataset', 'train.csv'), \n",
    "     os.path.join('.', 'generated_img_dataset', 'train.csv'), \n",
    "     os.path.join('.', 'generated_img_dataset', 'train_google')),\n",
    "\n",
    "    ('test',\n",
    "     os.path.join('.', 'original_ELCo_dataset', 'test.csv'), \n",
    "     os.path.join('.', 'generated_img_dataset', 'test.csv'), \n",
    "     os.path.join('.', 'generated_img_dataset', 'test_google')), \n",
    "\n",
    "    ('val',\n",
    "     os.path.join('.', 'original_ELCo_dataset', 'val.csv'), \n",
    "     os.path.join('.', 'generated_img_dataset', 'val.csv'), \n",
    "     os.path.join('.', 'generated_img_dataset', 'val_google')),\n",
    "]\n",
    "\n",
    "for folder_type, input, output, img_folder in csvs_to_generate:\n",
    "    generate_csv(folder_type, input, output, img_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of skipped_indices['train']: 57\n",
      "Length of skipped_indices['test']: 25\n",
      "Length of skipped_indices['val']: 10\n",
      "{'train': {1664, 386, 2309, 1542, 1031, 394, 2317, 2318, 2321, 402, 2324, 2326, 2328, 281, 1187, 422, 2343, 1448, 2346, 2348, 2351, 1586, 2356, 311, 1594, 2238, 2241, 1606, 1607, 971, 1109, 1111, 475, 1116, 1117, 1118, 1119, 1121, 1122, 1636, 1125, 1127, 1129, 1131, 1133, 1134, 367, 879, 880, 882, 1135, 1136, 1138, 1651, 2038, 1659, 2045}, 'test': {136, 26, 292, 42, 325, 335, 341, 342, 346, 349, 224, 225, 104, 106, 107, 108, 109, 362, 111, 492, 495, 114, 116, 246, 248}, 'val': {352, 354, 356, 200, 203, 23, 24, 155, 350, 351}}\n"
     ]
    }
   ],
   "source": [
    "for key, value in skipped_indices.items():\n",
    "    print(f\"Length of skipped_indices['{key}']: {len(value)}\")\n",
    "print(skipped_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensures csv rows match the image folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match for train: CSV rows = 2341, Images = 2341\n",
      "Match for test: CSV rows = 493, Images = 493\n",
      "Match for val: CSV rows = 384, Images = 384\n"
     ]
    }
   ],
   "source": [
    "for folder_type, _, output_csv, img_folder in csvs_to_generate:\n",
    "    # Count the number of rows in the CSV file\n",
    "    with open(output_csv, 'r', encoding='utf-8') as csv_file:\n",
    "        csv_row_count = sum(1 for _ in csv_file) - 1  # Subtract 1 for the header row\n",
    "\n",
    "    # Count the number of image files in the folder\n",
    "    img_file_count = len([f for f in os.listdir(img_folder) if os.path.isfile(os.path.join(img_folder, f))])\n",
    "\n",
    "    # Compare the counts\n",
    "    if csv_row_count != img_file_count:\n",
    "        print(f\"Mismatch for {folder_type}: CSV rows = {csv_row_count}, Images = {img_file_count}\")\n",
    "    else:\n",
    "        print(f\"Match for {folder_type}: CSV rows = {csv_row_count}, Images = {img_file_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
